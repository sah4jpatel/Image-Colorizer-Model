{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235c3ade",
   "metadata": {},
   "source": [
    "**Initial Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install matplotlib\n",
    "!pip install cv\n",
    "!pip install opencv-python\n",
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision \n",
    "import torchvision.transforms as T \n",
    "from PIL import Image\n",
    "import random\n",
    "import cv\n",
    "import os\n",
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "\n",
    "#img_dir = \"Deep Learning Data/ColorfulOriginal\" #change to desired dataset directory\n",
    "img_dir = \"Deep Learning Data/face_images\"\n",
    "#img_dir = \"Deep Learning Data\" #NOTE: make sure to change the number of augmentations (upperRange variable) from 7 to 4 otherwise the kernel will crash\n",
    "\n",
    "#out_dir = \"Colorful\" #change to desired output directory, make sure directories are cleared out and correspond to the \n",
    "out_dir = \"Faces\"\n",
    "#out_dir = \"All\"\n",
    "\n",
    "cpu_mode = False #true to use CPU instead of GPU\n",
    "fcm = False #false to avoid creating new image files\n",
    "rm = True #false to disable the regressor\n",
    "usm = False #true to use saved pretrained Model\n",
    "sm = True #false to disable saving model\n",
    "\n",
    "epoch_ct = 100\n",
    "\n",
    "if(cpu_mode):\n",
    "    epoch_ct = 10\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(\"File creation mode: \",str(fcm))\n",
    "print(\"Regressor mode: \", str(rm))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08352799",
   "metadata": {},
   "source": [
    "**Data Handling**\n",
    "\n",
    "Imports data from Deep Learning Data Folder and stores it as a data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, num):\n",
    "    images = []\n",
    "    for r, d, f in os.walk(folder):\n",
    "        for item in f:\n",
    "            if '.jpg' in item and '.icloud' not in item:\n",
    "                img = cv2.imread(os.path.join(r,item))\n",
    "                if img is not None:\n",
    "                    resize_width = 128\n",
    "                    resize_hieght = 128\n",
    "                    resized_dimensions = (resize_width, resize_hieght)\n",
    "                    resized_image = cv2.resize(img, resized_dimensions, interpolation=cv2.INTER_AREA)\n",
    "                    images.append(np.asarray(resized_image))\n",
    "                if fcm:\n",
    "                    ret = cv2.imwrite('%s/Augmented/%05d.jpg' % (out_dir,num), img)\n",
    "                num += 1  \n",
    "    return images\n",
    "\n",
    "def print_tensor_to_numpy(tensor):\n",
    "    tensor_reshape = torch.movedim(tensor, 0, 2)\n",
    "    npimg = tensor_reshape.numpy()\n",
    "    plt.imshow(npimg[:, :, :])  \n",
    "\n",
    "k = 0\n",
    "data = load_images_from_folder(img_dir, k)\n",
    "data = np.asarray(data)\n",
    "\n",
    "for i in range(0,9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(cv2.cvtColor(data[i][:,:,:],cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "data = torch.from_numpy(data) #converts numpy matrix to tensor\n",
    "data = torch.movedim(data, 3, 1) #moves the rgb to the second index\n",
    "data = data[torch.randperm(data.size()[0])]#randomly shuffles tensor along first dim \n",
    "\n",
    "print(\"Number of images loaded in: \",data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labConvHelper(input):\n",
    "    conv = cv2.cvtColor(input,cv2.COLOR_BGR2LAB)\n",
    "    return conv\n",
    "\n",
    "def convToLab_saveToFile(dataset, saveToFile = False, iter = 0):\n",
    "    output = []\n",
    "    outputl = []\n",
    "    outputa = []\n",
    "    outputb = []\n",
    "    for i in range(len(dataset)):\n",
    "        image = np.asarray(dataset[i,:,:,:])\n",
    "        image = np.swapaxes(image,1,0)\n",
    "        image = np.swapaxes(image,1,2)\n",
    "        imageLAB = labConvHelper(image)\n",
    "        l,a,b = cv2.split(imageLAB)\n",
    "        if saveToFile:\n",
    "            statusL = cv2.imwrite('%s/L/%05d_L.jpg' % (out_dir,iter), l)\n",
    "            statusa = cv2.imwrite('%s/a/%05d_a.jpg' % (out_dir,iter), a)\n",
    "            statusb = cv2.imwrite('%s/b/%05d_b.jpg' % (out_dir,iter), b)\n",
    "        iter += 1\n",
    "        output.append(imageLAB)\n",
    "        outputl.append(l)\n",
    "        outputa.append(a)\n",
    "        outputb.append(b)\n",
    "    return np.asarray(output),np.asarray(outputl),np.asarray(outputa),np.asarray(outputb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6e75d",
   "metadata": {},
   "source": [
    "**Augment Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img(tensor, show =False):\n",
    "    flipped_img = torch.flip(tensor, (2,))\n",
    "    if show == True:\n",
    "        print_tensor_to_numpy(flipped_img)\n",
    "    return flipped_img\n",
    "\n",
    "def crop_img(tensor, show = False):\n",
    "    crop = T.RandomCrop((96,96)) # transform for square crop\n",
    "    resize_crop = T.Resize((128,128)) #resizes image back to original dimension\n",
    "    cropped_img = resize_crop(crop(tensor)) \n",
    "    if show == True:\n",
    "        print_tensor_to_numpy(cropped_img)\n",
    "    return cropped_img\n",
    "\n",
    "def tint_img(tensor, show = False):\n",
    "    tint_val = 0.1*random.randrange(6, 10)\n",
    "    tensor[0, :, :] = tint_val*tensor[0, :, :]\n",
    "    tensor[1, :, :] = tint_val*tensor[1, :, :]\n",
    "    tensor[2, :, :] = tint_val*tensor[2, :, :]\n",
    "    tinted_img = tensor[:, :, :]\n",
    "    if show == True:\n",
    "        print_tensor_to_numpy(tinted_img)\n",
    "    return tinted_img\n",
    "\n",
    "def create_new_img(tensor, show = False):\n",
    "    chance = random.randrange(0, 10)\n",
    "    #60% chance of only one transformation\n",
    "    if (chance < 2):\n",
    "        new_img = flip_img(tensor) \n",
    "    elif (chance >= 2 and chance <4):\n",
    "        new_img = crop_img(tensor)\n",
    "    elif (chance >= 4 and chance <6):\n",
    "        new_img = tint_img(tensor)\n",
    "    #40% chance of more complex transformations\n",
    "    elif (chance >= 6 and chance <8):\n",
    "        tensor_cropped = crop_img(tensor)\n",
    "        tensor_tinted = tint_img(tensor_cropped)\n",
    "        new_img = tensor_tinted\n",
    "    elif (chance >= 8 and chance <=10):\n",
    "        tensor_flipped = flip_img(tensor) \n",
    "        tensor_cropped = crop_img(tensor_flipped)\n",
    "        tensor_tinted = tint_img(tensor_cropped)\n",
    "        new_img = tensor_tinted\n",
    "        \n",
    "    if show == True:\n",
    "        print_tensor_to_numpy(new_img)\n",
    "        \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image path to tensor converter\n",
    "def pathToTensor(inString):\n",
    "    inDat = labConvHelper(cv2.imread(inString))\n",
    "    inDat = np.swapaxes(inDat,1,2)\n",
    "    inDat = np.swapaxes(inDat,0,1)\n",
    "    tmpx, tmpa, tmpb= np.array_split(inDat,3)\n",
    "    tmpx = torch.from_numpy(tmpx[np.newaxis,:,:,:]/255).to(torch.float32).to(device)\n",
    "    return tmpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop which appends the newly created images (takes a while)\n",
    "k = len(data)\n",
    "k_old = len(data)\n",
    "\n",
    "upperRange = 7 #set this to generate data\n",
    "\n",
    "totalIter = k_old * (upperRange+1)-k_old\n",
    "bar = progressbar.ProgressBar(maxval=totalIter, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "for j in range(upperRange):\n",
    "    for i in range(0, k_old):\n",
    "        new_img = create_new_img(data[i, :, :, :])\n",
    "        new_img = new_img[None, :, :, :] #extending the dimensions so that vector can be appended\n",
    "        data = torch.cat((data, new_img), 0)\n",
    "        tmp = new_img.numpy()\n",
    "        tmp = tmp[0].swapaxes(0,1)\n",
    "        tmp = tmp.swapaxes(1,2)\n",
    "        if fcm:\n",
    "            ret = cv2.imwrite('%s/Augmented/%05d_A.jpg' % (out_dir,k), tmp)\n",
    "        if k-k_old < totalIter:\n",
    "            bar.update(k-k_old)\n",
    "        k += 1\n",
    "        \n",
    "bar.finish()\n",
    "        \n",
    "print(\"Done augmenting data...\")\n",
    "imageLab, l, a, b = convToLab_saveToFile(data, fcm, len(data))\n",
    "imageLab = np.swapaxes(imageLab,2,3)\n",
    "imageLab = np.swapaxes(imageLab,1,2)\n",
    "print(\"Done converting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data for regression\n",
    "# normData = torch.from_numpy(l).to(torch.float32)\n",
    "# normData = normData.unsqueeze(1)\n",
    "bs = 32\n",
    "x_set = l\n",
    "x_set = x_set[:,np.newaxis,:,:]\n",
    "\n",
    "x_set = x_set/255\n",
    "y_set = np.asarray([a,b])\n",
    "y_set = y_set.swapaxes(0,1)\n",
    "y_setR = np.expand_dims(np.expand_dims(y_set.mean(axis = 2).mean(axis = 2), axis = 2), axis = 2)\n",
    "norm_abR = y_setR/255\n",
    "norm_abC = y_set/255\n",
    "\n",
    "vIndex = int(np.ceil(len(x_set)*0.9))\n",
    "\n",
    "train_datasetR = torch.utils.data.TensorDataset(torch.from_numpy(x_set[0:vIndex]).to(torch.float32), torch.from_numpy(norm_abR[0:vIndex]).to(torch.float32))\n",
    "train_loaderR = torch.utils.data.DataLoader(dataset=train_datasetR, \n",
    "                                        batch_size=bs, \n",
    "                                        shuffle=True)\n",
    "\n",
    "validation_datasetR = torch.utils.data.TensorDataset(torch.from_numpy(x_set[vIndex:]).to(torch.float32), torch.from_numpy(norm_abR[vIndex:]).to(torch.float32))\n",
    "validation_loaderR = torch.utils.data.DataLoader(dataset=validation_datasetR,\n",
    "                                        batch_size=bs,\n",
    "                                        shuffle=True)\n",
    "\n",
    "train_datasetC = torch.utils.data.TensorDataset(torch.from_numpy(x_set[0:vIndex]).to(torch.float32), torch.from_numpy(norm_abC[0:vIndex]).to(torch.float32))\n",
    "train_loaderC = torch.utils.data.DataLoader(dataset=train_datasetC, \n",
    "                                        batch_size=bs, \n",
    "                                        shuffle=True)\n",
    "\n",
    "validation_datasetC = torch.utils.data.TensorDataset(torch.from_numpy(x_set[vIndex:]).to(torch.float32), torch.from_numpy(norm_abC[vIndex:]).to(torch.float32))\n",
    "validation_loaderC = torch.utils.data.DataLoader(dataset=validation_datasetC,\n",
    "                                        batch_size=bs,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb0a98",
   "metadata": {},
   "source": [
    "**Regressor**\n",
    "\n",
    "Finds mean chrominance across the entire greyscale image using spatial convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 128, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c1bn = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c2bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c3bn = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c4bn = nn.BatchNorm2d(16)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 16, out_channels = 8, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c5bn = nn.BatchNorm2d(8)\n",
    "        self.conv6 = nn.Conv2d(in_channels = 8, out_channels = 4, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c6bn = nn.BatchNorm2d(4)\n",
    "        self.conv7 = nn.Conv2d(in_channels = 4, out_channels = 2, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c7bn = nn.BatchNorm2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.c1bn(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.c2bn(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.c3bn(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.c4bn(x))\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.c5bn(x))\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.c6bn(x))\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(self.c7bn(x))\n",
    "        return x\n",
    "        \n",
    "if rm:\n",
    "    model = NN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    \n",
    "    for epoch in range(epoch_ct):\n",
    "        model.train()\n",
    "        loss = 0 \n",
    "        for i, (L_data, ab_data) in enumerate(train_loaderR):\n",
    "            model.train()\n",
    "            L_data = L_data.to(device)\n",
    "            ab_data = ab_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(L_data).to(device)\n",
    "            loss = criterion(outputs, ab_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        runningloss = 0\n",
    "        for j, (vL_data, vab_data) in enumerate(validation_loaderR):\n",
    "            vL_data = vL_data.to(device)\n",
    "            vab_data = vab_data.to(device)\n",
    "            voutputs = model(vL_data).to(device)\n",
    "            vloss = criterion(voutputs, vab_data)\n",
    "            runningloss += vloss.item()\n",
    "        vloss = runningloss/(j+1)\n",
    "        print ('Epoch [{}/{}], Training Loss: {}, Validation Loss: {}'.format(epoch+1, epoch_ct ,loss.item(), vloss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Regressor\n",
    "if rm:\n",
    "    model.eval()\n",
    "\n",
    "    inputData = x_set[128]\n",
    "\n",
    "    indata = torch.from_numpy(np.expand_dims(inputData,axis = 0)).to(device).type(torch.float32)\n",
    "    print(indata.dtype)\n",
    "    outReg = model(indata).to(device)\n",
    "    print(\"Results of regression: \", np.squeeze(outReg.detach().cpu().clone().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19988df9",
   "metadata": {},
   "source": [
    "**Colorizer**\n",
    "\n",
    "This is built on the previous regressor with extra upscaling and convolution layers in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-proof",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Model, training, and validation for the colorizer\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 128, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c1bn = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c2bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c3bn = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c4bn = nn.BatchNorm2d(16)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 16, out_channels = 8, kernel_size=3, stride = 2, padding = 1)\n",
    "        self.c5bn = nn.BatchNorm2d(8)\n",
    "        \n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.conv8 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.conv11 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=128, out_channels=2, kernel_size=3, stride = 1, padding = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.shape[2]\n",
    "        w = x.shape[3]\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.c1bn(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.c2bn(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.c3bn(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.c4bn(x))\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.c5bn(x))     \n",
    "        x = self.up(x)\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.up(x)\n",
    "        x = self.conv9(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.up(x)\n",
    "        x = self.conv10(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.up(x)\n",
    "        x = self.conv11(x)\n",
    "        #x = F.relu(x)\n",
    "        x = nn.Upsample(size = (h,w), mode = 'nearest')(x)\n",
    "        x = self.conv12(x)\n",
    "        #x = F.relu(x)\n",
    "        x = nn.Tanh()(x)\n",
    "        return x\n",
    "        \n",
    "if not usm:\n",
    "    model = NN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    \n",
    "    if os.path.exists(\"Saved Model\"):\n",
    "        model.load_state_dict(torch.load(\"Saved Model\"))\n",
    "\n",
    "    for epoch in range(epoch_ct):\n",
    "        model.train() \n",
    "        for i, (L_data, ab_data) in enumerate(train_loaderC):\n",
    "            L_data = L_data.to(device)\n",
    "            ab_data = ab_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(L_data).to(device)\n",
    "            loss = criterion(outputs, ab_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        runningloss = 0\n",
    "        for j, (vL_data, vab_data) in enumerate(validation_loaderC):\n",
    "            vL_data = vL_data.to(device)\n",
    "            vab_data = vab_data.to(device)\n",
    "            voutputs = model(vL_data).to(device)\n",
    "            vloss = criterion(voutputs, vab_data)\n",
    "            runningloss += vloss.item()\n",
    "        vloss = runningloss/(j+1)\n",
    "        print ('Epoch [{}/{}], Training Loss: {}, Validation Loss: {}'.format(epoch+1, epoch_ct ,loss.item(), vloss))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "if (not usm) and sm:\n",
    "    torch.save(model.state_dict(), \"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bba90a-ce04-47e4-88ac-aecdd02ef27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Model\n",
    "if usm:\n",
    "    model = NN().to(device)\n",
    "    model.load_state_dict(torch.load(\"Saved Model\"))\n",
    "    model.eval()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(20, 20),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "for i in range(3):\n",
    "    tmpx = torch.from_numpy(np.expand_dims(x_set[7**(i+1)],axis=0)).to(torch.float32).to(device)\n",
    "    model.eval()\n",
    "    out = np.expand_dims(model(tmpx).detach().cpu().clone().numpy(), axis = 0)\n",
    "    tmp = np.concatenate((tmpx.detach().cpu().clone().numpy()*100, out[0]*255-128), axis = 1)\n",
    "    pred = tmp[0]\n",
    "    known = imageLab[7**(i+1)]\n",
    "\n",
    "    tmpN = np.swapaxes(pred,0,1)\n",
    "    tmpN = np.swapaxes(tmpN,1,2)\n",
    "    tmp2 = np.swapaxes(known,0,1)\n",
    "    tmp2 = np.swapaxes(tmp2,1,2)\n",
    "    tmpN = (np.round(tmpN))\n",
    "\n",
    "    im = cv2.cvtColor(tmpN,cv2.COLOR_Lab2BGR)\n",
    "    im2 = cv2.cvtColor(tmp2, cv2.COLOR_Lab2BGR)\n",
    "\n",
    "    axs[i,0].imshow(cv2.cvtColor(im2,cv2.COLOR_BGR2RGB))\n",
    "    axs[i,1].imshow(cv2.cvtColor(im2,cv2.COLOR_BGR2LAB)[:,:,0], cmap='gray')\n",
    "    axs[i,2].imshow(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if i == 0:\n",
    "        axs[i,0].set_title('Actual Image')\n",
    "        axs[i,1].set_title('L* Channel')\n",
    "        axs[i,2].set_title('Predicted Image')\n",
    "\n",
    "path = \"Deep Learning Data/ColorfulOriginal/Banana/banana1.jpg\"\n",
    "\n",
    "inDat = labConvHelper(cv2.imread(path))\n",
    "inDat = np.swapaxes(inDat,1,2)\n",
    "inDat = np.swapaxes(inDat,0,1)\n",
    "tmpx, tmpa, tmpb= np.array_split(inDat,3)\n",
    "\n",
    "tmpx = torch.from_numpy(tmpx[np.newaxis,:,:,:]/255).to(torch.float32).to(device)\n",
    "out = model(tmpx).detach().cpu().clone().numpy()\n",
    "\n",
    "tmp = np.concatenate((tmpx[0].detach().cpu().clone().numpy()[np.newaxis,:,:,:]*100, out*255-128),axis = 1)\n",
    "pred = tmp[0]\n",
    "known = inDat\n",
    "\n",
    "tmpN = np.swapaxes(pred,0,1)\n",
    "tmpN = np.swapaxes(tmpN,1,2)\n",
    "tmp2 = np.swapaxes(known,0,1)\n",
    "tmp2 = np.swapaxes(tmp2,1,2)\n",
    "tmpN = (np.round(tmpN))\n",
    "\n",
    "im = cv2.cvtColor(tmpN,cv2.COLOR_Lab2BGR)\n",
    "im2 = cv2.cvtColor(tmp2, cv2.COLOR_Lab2BGR)\n",
    "\n",
    "axs[3,0].imshow(cv2.cvtColor(im2,cv2.COLOR_BGR2RGB))\n",
    "axs[3,1].imshow(cv2.cvtColor(im2, cv2.COLOR_BGR2LAB)[:,:,0], cmap='gray')\n",
    "axs[3,2].imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01072e-4e19-4ef0-bc49-0516778bf5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6801c18e8f98d131d679b452329ec4c6c6e2a07180fd911726b1fa2ebd3319cf"
  },
  "kernelspec": {
   "display_name": "PyTorch-1.10",
   "language": "python",
   "name": "pytorch-1.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
